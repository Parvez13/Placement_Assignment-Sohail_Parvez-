{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorch\n"
      ],
      "metadata": {
        "id": "fcIGgVLHa3L3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMDi7NFQaguY",
        "outputId": "7e68907f-5284-4f8b-8dd5-1a1863d51925"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, 32 * 7 * 7)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "nsASYIQ9bVlN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforms to tensor and normalize the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(), \n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "    ])"
      ],
      "metadata": {
        "id": "cKKee_GRbqMM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST dataset\n",
        "train_dataset = MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = MNIST(root='./data', train=False, transform=transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxiIZlAZb5w9",
        "outputId": "42b787e8-dacb-4dde-d10b-222bcde37571"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 225492936.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 114494984.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 64081125.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 22869782.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataloaders for train and test\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "cgVayz5lb-Lk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = CNN().to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    # Check if the desired accuracy is achieved\n",
        "    if accuracy >= 99.40:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlKbhciUcBwV",
        "outputId": "f4eb0da4-efa7-49af-a830-bf13ff270fa2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Validation Accuracy: 98.40%\n",
            "Epoch [2/10], Validation Accuracy: 98.80%\n",
            "Epoch [3/10], Validation Accuracy: 98.99%\n",
            "Epoch [4/10], Validation Accuracy: 98.88%\n",
            "Epoch [5/10], Validation Accuracy: 99.02%\n",
            "Epoch [6/10], Validation Accuracy: 99.05%\n",
            "Epoch [7/10], Validation Accuracy: 98.85%\n",
            "Epoch [8/10], Validation Accuracy: 99.03%\n",
            "Epoch [9/10], Validation Accuracy: 98.93%\n",
            "Epoch [10/10], Validation Accuracy: 99.01%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensorflow"
      ],
      "metadata": {
        "id": "NqqCn_OOdBPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Flatten,Dense\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train.reshape((-1, 28, 28, 1)).astype('float32') / 255.0\n",
        "x_test = x_test.reshape((-1, 28, 28, 1)).astype('float32') / 255.0\n",
        "\n",
        "\n",
        "# Define the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(32, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.fit(x_train, y_train, batch_size=64, epochs=1, verbose=1)\n",
        "\n",
        "    # Evaluation\n",
        "    _, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "    # Check if the desired accuracy is achieved\n",
        "    if accuracy >= 0.994:\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6W9GzVudCiD",
        "outputId": "8a2329eb-4f73-43af-e473-d8d82a37b59e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n",
            "938/938 [==============================] - 7s 4ms/step - loss: 0.1851 - accuracy: 0.9447\n",
            "Epoch [1/10], Validation Accuracy: 98.25%\n",
            "938/938 [==============================] - 3s 4ms/step - loss: 0.0576 - accuracy: 0.9826\n",
            "Epoch [2/10], Validation Accuracy: 98.45%\n",
            "938/938 [==============================] - 4s 4ms/step - loss: 0.0424 - accuracy: 0.9866\n",
            "Epoch [3/10], Validation Accuracy: 98.70%\n",
            "938/938 [==============================] - 3s 4ms/step - loss: 0.0323 - accuracy: 0.9898\n",
            "Epoch [4/10], Validation Accuracy: 98.86%\n",
            "938/938 [==============================] - 4s 4ms/step - loss: 0.0246 - accuracy: 0.9924\n",
            "Epoch [5/10], Validation Accuracy: 98.98%\n",
            "938/938 [==============================] - 3s 4ms/step - loss: 0.0205 - accuracy: 0.9934\n",
            "Epoch [6/10], Validation Accuracy: 99.01%\n",
            "938/938 [==============================] - 4s 4ms/step - loss: 0.0165 - accuracy: 0.9946\n",
            "Epoch [7/10], Validation Accuracy: 99.17%\n",
            "938/938 [==============================] - 3s 4ms/step - loss: 0.0136 - accuracy: 0.9955\n",
            "Epoch [8/10], Validation Accuracy: 99.06%\n",
            "938/938 [==============================] - 3s 4ms/step - loss: 0.0112 - accuracy: 0.9965\n",
            "Epoch [9/10], Validation Accuracy: 99.07%\n",
            "938/938 [==============================] - 4s 4ms/step - loss: 0.0088 - accuracy: 0.9971\n",
            "Epoch [10/10], Validation Accuracy: 99.18%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D6Sr2jEQduhn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}